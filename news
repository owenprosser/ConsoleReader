#!/usr/bin/env python3
import sys, tempfile, os, requests, json, re, asyncio, aiohttp, signal, webbrowser, subprocess
import urllib.request
from subprocess import call
from pick import pick
from bs4 import BeautifulSoup
from datetime import datetime, date
import calendar

class HackerNewsReader():
    def __init__(self):
        self.newsItems = []
        self.newsIDs = []
        self.noOfItems = 30
        self.browsedPosition = -1

    def PopulateArticlesList(self):
        r = requests.get('https://hacker-news.firebaseio.com//v0/topstories.json')
        self.newsIDs = json.loads(r.text)

    def PopulateNewsItems(self, startIndex):
        if(startIndex+self.noOfItems+1 > 500):
            startIndex = (500-self.noOfItems+1)

        IDlist = self.newsIDs[startIndex:startIndex+self.noOfItems+1]
        self.newsItems = []
        urls = []
        for id in IDlist:
            requestString = 'https://hacker-news.firebaseio.com/v0/item/'+str(id)+'.json'
            urls.append(requestString)
        client = HttpClient()
        data = asyncio.run(client.main(urls))

        for item in data:
            tempDict = dict()
            if "url" in item:
                tempDict.update({"id":item["id"], "title":item["title"], "url":item["url"]})
                self.newsItems.append(tempDict)

        tempDict.update({"id":0000, "title":"\tMore items:" + str(startIndex+1+self.noOfItems) +"-"+str(startIndex+(self.noOfItems*2)), "url":"http://owenprosser.com"})
        self.browsedPosition = startIndex

    def Menu(self):
        selected = None
        position = 0

        while True:
            if(position != self.browsedPosition):
                self.PopulateNewsItems(position)

            titleList = []
            for i in self.newsItems:
                titleList.append(i["title"])

            now = datetime.now()
            dayName = calendar.day_name[date.today().weekday()]
            day = now.day
            if 4 <= day <= 20 or 24 <= day <= 30:
                suffix = "th"
            else:
                suffix = ["st", "nd", "rd"][day % 10 - 1]

            title = "news.ycombinator.com - " + dayName + " "+ now.strftime("%d"+suffix) + now.strftime(" of %B %Y %H:%M")
            selected = pick(titleList, title, indicator='>')
            selectedUrl = self.newsItems[selected[1]]['url']

            if(selected[1] != len(titleList)-1):
                if "twitter.com" or "news.ycombinator.com" not in selectedUrl:
                    self.OpenTextInEditor(self.GetHtmlContent(selectedUrl))
                else:
                    if sys.platform == 'darwin':    # in case of OS X
                        subprocess.Popen(['open', selectedUrl])
                    else:
                        webbrowser.open_new_tab(selectedUrl)
            else:
                position += self.noOfItems

    def GetHtmlContent(self, url):
        text = ""
        uf = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        html = uf.content
        soup = BeautifulSoup(html, "lxml")

        for hit in soup.findAll(['h1', 'p']):
            hit = hit.text.strip()
            text += "\n\n" + hit

        return text

    def OpenTextInEditor(self, text):
        EDITOR = os.environ.get('EDITOR','vim')
        message = text.encode()
        with tempfile.NamedTemporaryFile(suffix=".tmp") as tf:
            tf.write(message)
            tf.flush()
            call([EDITOR, tf.name])

            # do the parsing with `tf` using regular File operations.
            # for instance:
            tf.seek(0)

class HttpClient():
    def __init__(self):
        self.url = "https://hacker-news.firebaseio.com/v0/item/"
        self.end = ".json"
        self.data = []

    async def get(self, url, session):
        try:
            async with session.get(url=url) as response:
                resp = await response.read()
                self.data.append(json.loads(resp))
        except Exception as e:
            print("Unable to get url {} due to {}.".format(url, e.__class__))


    async def main(self, urls):
        async with aiohttp.ClientSession() as session:
            ret = await asyncio.gather(*[self.get(url, session) for url in urls])
        return self.data

def signal_handler(sig, frame):
    sys.exit(0)

if __name__ == "__main__":
    signal.signal(signal.SIGINT, signal_handler)

    reader = HackerNewsReader()
    reader.PopulateArticlesList()

    reader.Menu()